{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Tune a Support Vector Machine\n",
    "description: |\n",
    "  In this post, we demonstrate how to optimize the hyperparameters of a support vector machine (SVM).\n",
    "categories:\n",
    "  - mlr3tuning\n",
    "  - tuning\n",
    "  - optimization\n",
    "author:\n",
    "  - name: Marc Becker\n",
    "date: 03-09-2021\n",
    "output:\n",
    "  distill::distill_article:\n",
    "    self_contained: false\n",
    "bibliography: bibliography.bib\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scope\n",
    "\n",
    "In this post, we demonstrate how to optimize the hyperparameters of a support vector machine (SVM).\n",
    "We are using the `r mlr_pkg(\"mlr3\")` machine learning framework with the `r mlr_pkg(\"mlr3tuning\")` extension package.\n",
    "\n",
    "First, we start by showing the basic building blocks of `r mlr_pkg(\"mlr3tuning\")`  and tune the `cost` and `gamma` hyperparameters of an SVM with a radial basis function on the [Iris data set](https://mlr3.mlr-org.com/reference/mlr_tasks_iris.html).\n",
    "After that, we use transformations to tune the `cost` hyperparameter on the logarithmic scale.\n",
    "Next, we explain the importance of dependencies to tune hyperparameters like `degree` which are dependent on the choice of kernel.\n",
    "After that, we fit an SVM with optimized hyperparameters on the full dataset.\n",
    "Finally, nested resampling is used to compute an unbiased performance estimate of our tuned SVM.\n",
    "\n",
    "# Prerequisites\n",
    "\n",
    "We load the `r mlr_pkg(\"mlr3verse\")`  package which pulls in the most important packages for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "message": false
   },
   "outputs": [],
   "source": [
    "library(mlr3verse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented.\n",
    "The [`lgr`](https://mlr3book.mlr-org.com/logging.html) package is used for logging in all `r mlr_pkg(\"mlr3\")` packages.\n",
    "The `r mlr_pkg(\"mlr3\")` logger prints the logging messages from the base package, whereas the `r mlr_pkg(\"bbotk\")`  logger is responsible for logging messages from the optimization packages (e.g. `r mlr_pkg(\"mlr3tuning\")` )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(7832)\n",
    "lgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\n",
    "lgr::get_logger(\"bbotk\")$set_threshold(\"warn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example, we use the [Iris data set](https://mlr3.mlr-org.com/reference/mlr_tasks_iris.html) which classifies 150 flowers in three species of Iris.\n",
    "The flowers are characterized by sepal length and width and petal length and width.\n",
    "The Iris data set allows us to quickly fit models to it.\n",
    "However, the influence of hyperparameter tuning on the predictive performance might be minor.\n",
    "Other data sets might give more meaningful tuning results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the task from mlr3\n",
    "task = tsk(\"iris\")\n",
    "\n",
    "# generate a quick textual overview using the skimr package\n",
    "skimr::skim(task$data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose the support vector machine implementation from the `r cran_pkg(\"e1071\")` package (which is based on [LIBSVM](https://www.csie.ntu.edu.tw/~cjlin/libsvm/)) and use it as a classification machine by setting `type` to `\"C-classification\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = lrn(\"classif.svm\", type = \"C-classification\", kernel = \"radial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Search Space\n",
    "\n",
    "For tuning, it is important to create a search space that defines the type and range of the hyperparameters.\n",
    "A learner stores all information about its hyperparameters in the slot `$param_set`.\n",
    "Not all parameters are tunable. \n",
    "We have to choose a subset of the hyperparameters we want to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(learner$param_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `r ref(\"to_tune()\")` function to define the range over which the hyperparameter should be tuned.\n",
    "We opt for the `cost` and `gamma` hyperparameters of the `radial` kernel and set the tuning ranges with lower and upper bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner$param_set$values$cost = to_tune(0.1, 10)\n",
    "learner$param_set$values$gamma = to_tune(0, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning\n",
    "\n",
    "We specify how to evaluate the performance of the different hyperparameter configurations.\n",
    "For this, we choose 3-fold cross validation as the resampling strategy and the classification error as the performance measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampling = rsmp(\"cv\", folds = 3)\n",
    "measure = msr(\"classif.ce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, we have to select a budget for the tuning.\n",
    "This is done by choosing a `r ref(\"Terminator\")`, which stops the tuning e.g. after a performance level is reached or after a given time.\n",
    "However, some tuners like grid search terminate themselves.\n",
    "In this case, we  choose a terminator that never stops and the tuning is not stopped before all grid points are evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminator = trm(\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we can construct a `r ref(\"TuningInstanceSingleCrit\")` that describes the tuning problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = TuningInstanceSingleCrit$new(\n",
    "  task = task,\n",
    "  learner = learner,\n",
    "  resampling = resampling,\n",
    "  measure = measure,\n",
    "  terminator = terminator\n",
    ")\n",
    "\n",
    "print(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have to choose a `r ref(\"Tuner\")`.\n",
    "[Grid Search](https://mlr3tuning.mlr-org.com/reference/mlr_tuners_grid_search.html) discretizes numeric parameters into a given resolution and constructs a grid from the Cartesian product of these sets. \n",
    "Categorical parameters produce a grid over all levels specified in the search space.\n",
    "In this example, we only use a resolution of 5 to keep the runtime low.\n",
    "Usually, a higher resolution is used to create a denser grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = tnr(\"grid_search\", resolution = 5)\n",
    "\n",
    "print(tuner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can preview the proposed configurations by using `r ref(\"generate_design_grid()\")`. This function is internally executed by `r ref(\"TunerGridSearch\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_design_grid(learner$param_set$search_space(), resolution = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trigger the tuning by passing the `r ref(\"TuningInstanceSingleCrit\")` to the `$optimize()` method of the `r ref(\"Tuner\")`. The instance is modified in-place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "instance = readRDS(\"data/instance_1.rda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "tuner$optimize(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the performances depending on the evaluated `cost` and `gamma` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "autoplot(instance, type = \"surface\", cols_x = c(\"cost\", \"gamma\"), learner = lrn(\"regr.km\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "# regr.km prints a log\n",
    "log = capture.output(autoplot(instance, type = \"surface\", cols_x = c(\"cost\", \"gamma\"), learner = lrn(\"regr.km\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "The points mark the evaluated `cost` and `gamma` values.\n",
    "We should not infer the performance of new values from the heatmap since it is only an interpolation.\n",
    "However, we can see the general interaction between the hyperparameters.\n",
    "\n",
    "Tuning a learner can be shortened by using the `r ref(\"tune()\")`-shortcut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "learner = lrn(\"classif.svm\", type = \"C-classification\", kernel = \"radial\")\n",
    "learner$param_set$values$cost = to_tune(0.1, 10)\n",
    "learner$param_set$values$gamma = to_tune(0, 5)\n",
    "\n",
    "instance = tune(\n",
    "  method = \"grid_search\", \n",
    "  task = tsk(\"iris\"), \n",
    "  learner = learner, \n",
    "  resampling = rsmp (\"holdout\"), \n",
    "  measure = msr(\"classif.ce\"),\n",
    "  resolution = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation\n",
    "\n",
    "Next, we want to tune the `cost` and `gamma` hyperparameter more efficiently.\n",
    "It is recommended to tune `cost` and `gamma` on the logarithmic scale [@hsuPracticalGuideSupport2003].\n",
    "The log transformation emphasizes smaller `cost` and `gamma` values but also creates large values.\n",
    "Therefore, we use a log transformation to emphasize this region of the search space with a denser grid.\n",
    "\n",
    "Generally speaking, transformations can be used to convert hyperparameters to a new scale.\n",
    "These transformations are applied before the proposed configuration is passed to the `r ref(\"Learner\")`.\n",
    "We can directly define the transformation in the `r ref(\"to_tune()\")` function.\n",
    "The lower and upper bound is set on the original scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = lrn(\"classif.svm\", type = \"C-classification\", kernel = \"radial\")\n",
    "\n",
    "# tune from 2^-15 to 2^15 on a log scale\n",
    "learner$param_set$values$cost = to_tune(p_dbl(-15, 15, trafo = function(x) 2^x))\n",
    "\n",
    "# tune from 2^-15 to 2^5 on a log scale\n",
    "learner$param_set$values$gamma = to_tune(p_dbl(-15, 5, trafo = function(x) 2^x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations to the log scale are the ones most commonly used. \n",
    "We can use a shortcut for this transformation.\n",
    "The lower and upper bound is set on the transformed scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner$param_set$values$cost = to_tune(p_dbl(1e-5, 1e5, logscale = TRUE))\n",
    "learner$param_set$values$gamma = to_tune(p_dbl(1e-5, 1e5, logscale = TRUE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new `r ref(\"TuningInstanceSingleCrit\")` and trigger the tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "instance = readRDS(\"data/instance_2.rda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "instance = tune(\n",
    "  method = \"grid_search\", \n",
    "  task = task, \n",
    "  learner = learner, \n",
    "  resampling = resampling, \n",
    "  measure = measure,\n",
    "  resolution = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameter values after the transformation are stored in the `x_domain` column as lists.\n",
    "We can expand these lists into multiple columns by using `as.data.table()`.\n",
    "The hyperparameter names are prefixed by `x_domain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = as.data.table(instance$archive)\n",
    "data[, .(cost, gamma, x_domain_cost, x_domain_gamma)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the performances depending on the evaluated `cost` and `gamma` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ggplot2)\n",
    "library(scales)\n",
    "autoplot(instance, type = \"points\", cols_x = c(\"x_domain_cost\", \"x_domain_gamma\")) +\n",
    "  scale_x_continuous(\n",
    "    trans = log2_trans(),\n",
    "    breaks = trans_breaks(\"log10\", function(x) 10^x),\n",
    "    labels = trans_format(\"log10\", math_format(10^.x))) +\n",
    "  scale_y_continuous(\n",
    "    trans = log2_trans(),\n",
    "    breaks = trans_breaks(\"log10\", function(x) 10^x),\n",
    "    labels = trans_format(\"log10\", math_format(10^.x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies\n",
    "\n",
    "Dependencies ensure that certain parameters are only proposed depending on values of other hyperparameters.\n",
    "We want to tune the `degree` hyperparameter that is only needed for the `polynomial` kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = lrn(\"classif.svm\", type = \"C-classification\")\n",
    "\n",
    "learner$param_set$values$cost = to_tune(p_dbl(1e-5, 1e5, logscale = TRUE))\n",
    "learner$param_set$values$gamma = to_tune(p_dbl(1e-5, 1e5, logscale = TRUE))\n",
    "\n",
    "learner$param_set$values$kernel = to_tune(c(\"polynomial\", \"radial\"))\n",
    "learner$param_set$values$degree = to_tune(1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dependencies are already stored in the learner parameter set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner$param_set$deps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `gamma` hyperparameter depends on the kernel being `polynomial`, `radial` or `sigmoid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner$param_set$deps$cond[[5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "whereas the `degree` hyperparameter is solely used by the `polynomial` kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner$param_set$deps$cond[[3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We preview the grid to show the effect of the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_design_grid(learner$param_set$search_space(), resolution = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value for `degree` is `NA` if the dependency on the `kernel` is not satisfied.\n",
    "\n",
    "We create a new `r ref(\"TuningInstanceSingleCrit\")` and trigger the tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "instance = readRDS(\"data/instance_3.rda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "instance = tune(\n",
    "  method = \"grid_search\", \n",
    "  task = task, \n",
    "  learner = learner, \n",
    "  resampling = resampling, \n",
    "  measure = measure,\n",
    "  resolution = 3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance$result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model\n",
    "\n",
    "We add the optimized hyperparameters to the learner and train the learner on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = lrn(\"classif.svm\")\n",
    "learner$param_set$values = instance$result_learner_param_vals\n",
    "learner$train(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained model can now be used to make predictions on new data.\n",
    "A common mistake is to report the performance estimated on the resampling sets on which the tuning was performed (`instance$result_y`) as the model's performance.\n",
    "These scores might be biased and overestimate the ability of the fitted model to predict with new data.\n",
    "Instead, we have to use nested resampling to get an unbiased performance estimate.\n",
    "\n",
    "# Nested Resampling\n",
    "\n",
    "Tuning should not be performed on the same resampling sets which are used for evaluating the model itself, since this would result in a biased performance estimate.\n",
    "[Nested resampling](https://mlr3book.mlr-org.com/nested-resampling.html) uses an outer and inner resampling to separate the tuning from the performance estimation of the model.\n",
    "We can use the `r ref(\"AutoTuner\")` class for running nested resampling.\n",
    "The `r ref(\"AutoTuner\")` wraps a `r ref(\"Learner\")` and tunes the hyperparameter of the learner during `$train()`.\n",
    "This is our inner resampling loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = lrn(\"classif.svm\", type = \"C-classification\")\n",
    "learner$param_set$values$cost = to_tune(p_dbl(1e-5, 1e5, logscale = TRUE))\n",
    "learner$param_set$values$gamma = to_tune(p_dbl(1e-5, 1e5, logscale = TRUE))\n",
    "learner$param_set$values$kernel = to_tune(c(\"polynomial\", \"radial\"))\n",
    "learner$param_set$values$degree = to_tune(1, 4)\n",
    "\n",
    "resampling_inner = rsmp(\"cv\", folds = 3)\n",
    "terminator = trm(\"none\")\n",
    "tuner = tnr(\"grid_search\", resolution = 3)\n",
    "\n",
    "at = AutoTuner$new(\n",
    "  learner = learner,\n",
    "  resampling = resampling_inner,\n",
    "  measure = measure,\n",
    "  terminator = terminator,\n",
    "  tuner = tuner,\n",
    "  store_models = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We put the `r ref(\"AutoTuner\")` into a `r ref(\"resample()\")` call to get the outer resampling loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "rr = readRDS(\"data/rr_1.rda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "resampling_outer = rsmp(\"cv\", folds = 3)\n",
    "rr = resample(task = task, learner = at, resampling = resampling_outer, store_models = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the inner tuning results for stable hyperparameters.\n",
    "This means that the selected hyperparameters should not vary too much.\n",
    "We might observe unstable models in this example because the small data set and the low number of resampling iterations might introduce too much randomness.\n",
    "Usually, we aim for the selection of stable hyperparameters for all outer training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_inner_tuning_results(rr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to compare the predictive performances estimated on the outer resampling to the inner resampling (`extract_inner_tuning_results(rr)`).\n",
    "Significantly lower predictive performances on the outer resampling indicate that the models with the optimized hyperparameters overfit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr$score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The archive of the `r ref(\"AutoTuner\")`s allows us to inspect all evaluated hyperparameters configurations with the associated predictive performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr$learners[[1]]$archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aggregated performance of all outer resampling iterations is essentially the unbiased performance of an SVM with optimal hyperparameter found by grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr$aggregate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying nested resampling can be shortened by using the `r ref(\"tune_nested()\")`-shortcut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "learner = lrn(\"classif.svm\", type = \"C-classification\")\n",
    "learner$param_set$values$cost = to_tune(p_dbl(1e-5, 1e5, logscale = TRUE))\n",
    "learner$param_set$values$gamma = to_tune(p_dbl(1e-5, 1e5, logscale = TRUE))\n",
    "learner$param_set$values$kernel = to_tune(c(\"polynomial\", \"radial\"))\n",
    "learner$param_set$values$degree = to_tune(1, 4)\n",
    "\n",
    "rr = tune_nested(\n",
    "  method = \"grid_search\",\n",
    "  task = tsk(\"iris\"),\n",
    "  learner = learner, \n",
    "  inner_resampling = rsmp (\"cv\", folds = 3),\n",
    "  outer_resampling = rsmp(\"cv\", folds = 3), \n",
    "  measure = msr(\"classif.ce\"),\n",
    "  resolution = 3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "The [mlr3book](https://mlr3book.mlr-org.com/) includes chapters on [tuning spaces](https://mlr3book.mlr-org.com/searchspace.html) and [hyperparameter tuning](https://mlr3book.mlr-org.com/tuning.html). \n",
    "The [mlr3cheatsheets](https://cheatsheets.mlr-org.com/) contain frequently used commands and workflows of mlr3.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "message,name,tags,eval,-all",
   "main_language": "R",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}